{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a class of feedforward artificial neural network (ANN) that consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The key characteristic of an MLP is that it is fully connected, meaning that each node in one layer connects with a certain weight to every node in the following layer. This architecture comprises multiple layers through which data is processed sequentially:\n",
    "\n",
    "- **Input Layer**: Receives the input features. It acts as the entry point that transfers data to the hidden layers.\n",
    "- **Hidden Layers**: The layers in between input and output layers where all the computation is done. Can have one or more hidden layers.\n",
    "- **Output Layer**: The final layer that produces the output of the model. The function used in the output layer depends on the nature of the prediction task.\n",
    "\n",
    "## How MLPs Work\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Data is passed through the network from the input layer to the hidden layers and finally to the output layer. Each neuron in the hidden layers transforms the values from the previous layer with a weighted linear summation followed by a non-linear activation function like sigmoid, tanh, or ReLU.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Once the output is obtained, MLP calculates the error in prediction, which is the difference between the actual output and the predicted output. The model then adjusts the weights in reverse from output to input by propagating the error back through the network layers. This process minimizes the error in predictions using gradient descent or other optimization techniques.\n",
    "\n",
    "## Applications of MLP\n",
    "\n",
    "MLPs are versatile and can be applied to various domains such as:\n",
    "\n",
    "- **Image Recognition**: MLPs can classify images, recognize patterns or features.\n",
    "- **Speech Recognition**: They are used to recognize spoken words and convert the audio signals into text.\n",
    "- **Financial Forecasting**: MLPs help in predicting stock prices, risk management, and other financial attributes.\n",
    "- **Medical Diagnosis**: They assist in diagnosing diseases based on symptoms and patient data.\n",
    "- **Natural Language Processing**: MLPs are used in sentiment analysis, topic classification, and other text-related tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Multi-Layer Perceptron (MLP)\n",
    "\n",
    "This notebook demonstrates the application of a Multi-Layer Perceptron (MLP), a type of neural network, to perform sentiment analysis on the IMDB movie reviews dataset. We aim to classify reviews as positive or negative based on their content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "\n",
    "First, we'll load the dataset and explore its structure and some basic statistics to understand the data better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('C:/Users/78641/Downloads/IMDB Dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display general information about the dataframe\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The text data needs to be converted into a format that can be interpreted by the MLP model. We'll clean and vectorize the text data, and encode the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the text data\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "X = vectorizer.fit_transform(data['review'])\n",
    "\n",
    "# Encoding the labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data['sentiment'])\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the MLP Model\n",
    "\n",
    "We will configure a Multi-Layer Perceptron with a single hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the MLP Classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, activation='relu', solver='adam', random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now, let's train the MLP model on the preprocessed movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=300, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=300, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(max_iter=300, random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the MLP model\n",
    "mlp.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the model's performance on the unseen test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8493\n",
      "Confusion Matrix:\n",
      " [[4219  742]\n",
      " [ 765 4274]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      4961\n",
      "           1       0.85      0.85      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Analysis\n",
    "\n",
    "## Overview\n",
    "The MLP model achieved an accuracy of 84.93% on the IMDB movie review sentiment analysis task. This performance is commendable and indicates that the model is generally effective in distinguishing between positive and negative reviews.\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Accuracy\n",
    "- **Overall Accuracy**: 84.93%\n",
    "  - The model correctly predicted the sentiment of approximately 85% of the reviews in the test set.\n",
    "\n",
    "### Precision and Recall\n",
    "- **Precision** (Positive Predictive Value):\n",
    "  - **Class 0 (Negative Reviews)**: 85%\n",
    "  - **Class 1 (Positive Reviews)**: 85%\n",
    "- **Recall** (True Positive Rate):\n",
    "  - **Class 0 (Negative Reviews)**: 85%\n",
    "  - **Class 1 (Positive Reviews)**: 85%\n",
    "\n",
    "These metrics indicate that the model is equally skilled at identifying both negative and positive reviews. The balance between precision and recall suggests that the model does not overly favor one class over the other.\n",
    "\n",
    "### F1-Score\n",
    "- **Class 0 (Negative Reviews)**: 85%\n",
    "- **Class 1 (Positive Reviews)**: 85%\n",
    "  \n",
    "The F1-score, which balances precision and recall, is also 85% for both classes, further confirming the model's balanced performance across both categories of sentiment.\n",
    "\n",
    "## Confusion Matrix Analysis\n",
    "The confusion matrix provides further insight into the model's performance:\n",
    "\n",
    "- **True Positives for Class 1 (Positive Reviews)**: 4274\n",
    "- **True Negatives for Class 0 (Negative Reviews)**: 4219\n",
    "- **False Positives**: 742 (Negative reviews incorrectly classified as Positive)\n",
    "- **False Negatives**: 765 (Positive reviews incorrectly classified as Negative)\n",
    "\n",
    "The relatively similar numbers of false positives and false negatives suggest that the model's errors are evenly distributed across both classes.\n",
    "\n",
    "## Conclusion\n",
    "The MLP model demonstrates strong performance in classifying the sentiment of IMDB movie reviews. With an overall accuracy of nearly 85% and balanced precision, recall, and F1-scores across both sentiment classes, the model proves effective for this binary classification task. Future work could explore model optimization techniques such as adjusting network architecture, experimenting with different activation functions, or employing more sophisticated natural language processing techniques like word embeddings to potentially boost performance further.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
